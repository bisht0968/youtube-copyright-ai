{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b444ef83-d466-4d1a-b77e-883c5f3d70ba",
   "metadata": {},
   "source": [
    "Installing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf696138-461b-4044-b811-e3932f041602",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-api-python-client\n",
    "!pip install --upgrade google-api-python-client google-auth google-auth-oauthlib google-auth-httplib2 tqdm pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8758db45-ad89-4796-ae73-89964a364339",
   "metadata": {},
   "source": [
    "Confirming the Google API Client installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1864c1e3-0a46-4dd5-82db-14cb1324c00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import googleapiclient.discovery\n",
    "print(\"Google API Client Installed Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2158bc32-8e28-4477-9e51-46998ee17bae",
   "metadata": {},
   "source": [
    "Initializing Youtube API Key generated from Google Cloud Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16448bff-2d86-4588-b337-1b977728046d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import googleapiclient.discovery\n",
    "\n",
    "# Replace with your own API key\n",
    "API_KEY = \"AIzaSyC-rQkvH_377WDr4e9y0005Dk9aCdonKhg\"\n",
    "\n",
    "# Build the YouTube API client\n",
    "youtube = googleapiclient.discovery.build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "print(\"YouTube API client initialized successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e467fc-acae-4cf6-8b90-391b3e9a12a8",
   "metadata": {},
   "source": [
    "Fetched Youtube-8M Dataset features from Google Youtube API \n",
    "(3287 rows) have been fetched throught the API having features:\n",
    "('video_id', 'title', 'description', 'published_at', 'view_count', 'like_count', 'comment_count) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3a7dfe-4f25-4057-a4f4-9eb0b06ab1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from googleapiclient.discovery import build\n",
    "import requests\n",
    "\n",
    "# ===========================\n",
    "# âœ… 1. SETUP YOUTUBE API\n",
    "# ===========================\n",
    "API_KEY = \"AIzaSyAva4WwJSmrTtxu25csmwA5kmdLb7Ob1Qk\"  # Replace with your API Key\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "# ===========================\n",
    "# âœ… 2. DEFINE SEARCH QUERIES\n",
    "# ===========================\n",
    "queries = [\n",
    "    \"Copyright Claims\",\n",
    "    \"Copyright Strike YouTube\",\n",
    "    \"YouTube Fair Use\",\n",
    "    \"Copyright Infringement\",\n",
    "    \"YouTube Monetization Issues\",\n",
    "    \"YouTube Copyright Takedown\",\n",
    "    \"DMCA Takedown YouTube\"\n",
    "]\n",
    "\n",
    "# ===========================\n",
    "# âœ… 3. FETCH VIDEO IDS (2,500+)\n",
    "# ===========================\n",
    "def get_video_ids(query, max_results=50, total_videos=500):\n",
    "    video_ids = set()\n",
    "    next_page_token = None\n",
    "\n",
    "    while len(video_ids) < total_videos:\n",
    "        request = youtube.search().list(\n",
    "            q=query, part=\"id\", type=\"video\",\n",
    "            maxResults=max_results, pageToken=next_page_token\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        for item in response.get(\"items\", []):\n",
    "            video_ids.add(item[\"id\"][\"videoId\"])\n",
    "\n",
    "        next_page_token = response.get(\"nextPageToken\")\n",
    "        if not next_page_token:\n",
    "            break  # Stop if no more pages available\n",
    "\n",
    "        time.sleep(1)  # Avoid API quota limits\n",
    "\n",
    "    return list(video_ids)[:total_videos]\n",
    "\n",
    "video_ids = set()\n",
    "for query in queries:\n",
    "    video_ids.update(get_video_ids(query, max_results=50, total_videos=500))\n",
    "\n",
    "video_ids = list(video_ids)  # Remove duplicates\n",
    "print(f\"âœ… Collected {len(video_ids)} unique video IDs!\")\n",
    "\n",
    "# ===========================\n",
    "# âœ… 4. FETCH VIDEO DETAILS\n",
    "# ===========================\n",
    "def get_video_details(video_ids):\n",
    "    video_data = []\n",
    "    \n",
    "    for i in range(0, len(video_ids), 50):  # YouTube API allows max 50 videos per request\n",
    "        request = youtube.videos().list(\n",
    "            part=\"snippet,statistics\",\n",
    "            id=\",\".join(video_ids[i:i+50])\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        for item in response[\"items\"]:\n",
    "            video_info = {\n",
    "                \"video_id\": item[\"id\"],\n",
    "                \"title\": item[\"snippet\"][\"title\"],\n",
    "                \"description\": item[\"snippet\"][\"description\"],\n",
    "                \"published_at\": item[\"snippet\"][\"publishedAt\"],\n",
    "                \"view_count\": item[\"statistics\"].get(\"viewCount\", 0),\n",
    "                \"like_count\": item[\"statistics\"].get(\"likeCount\", 0),\n",
    "                \"comment_count\": item[\"statistics\"].get(\"commentCount\", 0)\n",
    "            }\n",
    "            video_data.append(video_info)\n",
    "\n",
    "        time.sleep(1)  # Avoid API quota limit\n",
    "\n",
    "    return pd.DataFrame(video_data)\n",
    "\n",
    "df = get_video_details(video_ids)\n",
    "print(f\"âœ… Collected {len(df)} rows!\")\n",
    "\n",
    "# Save dataset\n",
    "df.to_csv(\"youtube_copyright_data.csv\", index=False)\n",
    "print(\"âœ… YouTube dataset saved successfully!\")\n",
    "\n",
    "# ===========================\n",
    "# âœ… 5. DOWNLOAD YOUTUBE UGC DATASET\n",
    "# ===========================\n",
    "UGC_DATASET_URL = \"https://storage.googleapis.com/ugc-dataset-public/YouTube-UGC.zip\"\n",
    "UGC_DATASET_PATH = \"YouTube-UGC.zip\"\n",
    "\n",
    "print(\"ðŸ“¥ Downloading YouTube-UGC dataset...\")\n",
    "response = requests.get(UGC_DATASET_URL, stream=True)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    with open(UGC_DATASET_PATH, \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=1024):\n",
    "            f.write(chunk)\n",
    "    print(\"âœ… YouTube-UGC dataset downloaded successfully!\")\n",
    "else:\n",
    "    print(\"âŒ Failed to download YouTube-UGC dataset.\")\n",
    "\n",
    "print(\"ðŸŽ¯ All tasks completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5ab872-03d2-4a4d-a12a-df324c4f8298",
   "metadata": {},
   "source": [
    "Installing other required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854d7a3e-5ad6-4a99-a7b5-ca37c5e6e537",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312464cb-acde-4ceb-a611-2dce58a7cc28",
   "metadata": {},
   "source": [
    "Fetching other features from Youtube API and merging it with the existing dataset to make it a more strong dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce154d73-2a8a-4026-8c9b-38727560c677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "import librosa\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Load existing dataset (Replace 'your_dataset.csv' with your actual file)\n",
    "df = pd.read_csv(\"youtube_copyright_data.csv\")\n",
    "\n",
    "# Initialize sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Your YouTube API Key\n",
    "API_KEY = \"AIzaSyAva4WwJSmrTtxu25csmwA5kmdLb7Ob1Qk\"\n",
    "\n",
    "# Function to fetch video metadata\n",
    "def get_video_metadata(video_id):\n",
    "    url = f\"https://www.googleapis.com/youtube/v3/videos?part=snippet,contentDetails&id={video_id}&key={API_KEY}\"\n",
    "    response = requests.get(url).json()\n",
    "    if \"items\" in response and len(response[\"items\"]) > 0:\n",
    "        video = response[\"items\"][0]\n",
    "        return {\n",
    "            \"tags\": \", \".join(video[\"snippet\"].get(\"tags\", [])),\n",
    "            \"category\": video[\"snippet\"][\"categoryId\"],\n",
    "            \"duration\": video[\"contentDetails\"].get(\"duration\", \"N/A\"),\n",
    "            \"channel_name\": video[\"snippet\"][\"channelTitle\"]\n",
    "        }\n",
    "    return {\"tags\": \"\", \"category\": \"\", \"duration\": \"\", \"channel_name\": \"\"}\n",
    "\n",
    "# Function to extract image features from thumbnail\n",
    "def extract_image_features(thumbnail_url):\n",
    "    try:\n",
    "        response = requests.get(thumbnail_url)\n",
    "        img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "        img_array = np.array(img)\n",
    "        brightness = np.mean(img_array)  # Average pixel intensity\n",
    "        return brightness\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Function for sentiment analysis of comments\n",
    "def get_sentiment_score(comment_text):\n",
    "    return analyzer.polarity_scores(str(comment_text))['compound']\n",
    "\n",
    "# Function to extract audio features\n",
    "def extract_audio_features(audio_file):\n",
    "    try:\n",
    "        y, sr = librosa.load(audio_file)\n",
    "        pitch = librosa.feature.spectral_centroid(y=y, sr=sr).mean()\n",
    "        return pitch\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Add new features to the dataset\n",
    "metadata_list = []\n",
    "brightness_list = []\n",
    "sentiment_scores = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    video_id = row[\"video_id\"]\n",
    "    metadata = get_video_metadata(video_id)\n",
    "    metadata_list.append(metadata)\n",
    "\n",
    "    thumbnail_url = f\"https://img.youtube.com/vi/{video_id}/hqdefault.jpg\"\n",
    "    brightness = extract_image_features(thumbnail_url)\n",
    "    brightness_list.append(brightness)\n",
    "\n",
    "    sentiment = get_sentiment_score(row[\"comment_count\"])\n",
    "    sentiment_scores.append(sentiment)\n",
    "\n",
    "# Convert metadata list to DataFrame and merge with original dataset\n",
    "metadata_df = pd.DataFrame(metadata_list)\n",
    "df = pd.concat([df, metadata_df], axis=1)\n",
    "\n",
    "# Add image brightness and sentiment score\n",
    "df[\"brightness\"] = brightness_list\n",
    "df[\"sentiment_score\"] = sentiment_scores\n",
    "\n",
    "# Save enhanced dataset\n",
    "df.to_csv(\"enhanced_youtube_dataset.csv\", index=False)\n",
    "\n",
    "print(\"âœ… Enhanced dataset saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a951342d-e50c-421f-9787-fc9b32eb3a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
